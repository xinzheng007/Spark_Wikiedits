{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering on Text Data\n",
    "\n",
    "In this notebook, we calculate features on data streamed from seppe.net in Preprocessing.ipynb. We calculate the following features on the data and columns in the extracted wiki_df dataframe:\n",
    "\n",
    "- TF-IDF: Term Frequency - Inverse Document Frequency matrix is a feature which measures the occurrence of words normalized by their overall occurrence in the entire document corpus. We use this on the raw edits applied to each Wikipedia article to help gather features as to which words and terms in overall edits may lead to vandal edits or otherwise.\n",
    "- LDA: Latent Dirichlet Analysis is a technique used in automated topic discovery. We use this on the overall Wiki text before edit to discover the original topic of the piece. The reason for using this feature is that some topics may be more susceptible to vandalism than others, such as political articles, for example.\n",
    "- Leichtenstein Distance: This is used again on the raw edits to quantify the size of the edit. Usually large edits might correspond to large erasures or changes in a document text indicating vandalism and censoring of data from the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the feature transformation classes for doing TF-IDF \n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, CountVectorizer, IDF, NGram\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Machine learning libraries\n",
    "from pyspark.ml.tuning import TrainValidationSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load output from *preprocessing.ipynb*\n",
    "\n",
    "you can either \n",
    " - **option A**: run `preprocessing.ipynb` and perform the steps below to end up to the `final_df`\n",
    " - **option B** just import the data via: `spark.read.parquet(\"../output/output_preprocessing.parquet\")`\n",
    " \n",
    "### TODO\n",
    " \n",
    " - Evaluate feature engineering steps on a larget part of the data, the subset dataset containg approx. 800 observations and approx 300 observations after downsampling.\n",
    " - Train `final model` on **all data**.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  %run \"preprocessing.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"../data/subset/*\" (this is the path to the subset data)\n",
    "\n",
    "# wiki_df = get_wiki_df(path=\"../data/subset/*\")\n",
    "\n",
    "# clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# #In order to get the actual difference column\n",
    "# df_with_difference = get_difference_column(clean_df)\n",
    "# final_df = split_difference_into_removed_added(df_with_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B\n",
    "\n",
    "!! NOTE !! this is only on a subset of the data (800 observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = spark.read.parquet(\"../output/output_preprocessing.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Balance data using *stratified sampling*\n",
    "\n",
    "We do this to ease the memory usage of the TF-IDF. In any case, the data is highly imbalanced, with a current distribution of:\n",
    "\n",
    "- safe: 30333 (~86%)\n",
    "- unsafe: 4136 (~13.2%)\n",
    "- vandal: 270 (~0.8%)\n",
    "\n",
    "It is better to rebalance this by **downsampling** the `safe` class and keeping the others.\n",
    "\n",
    "**TODO**\n",
    " - check effect on performance: does it improves/worsens or really doesn't matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratified_sample(df, fractions, categorical_class=\"label\", random_state = 42):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function creates a stratified sample based on thresholds specified on a categorical class\n",
    "    The aim of this is to balance a dataset more evenly by reducing the size of over-prepresented classes.\n",
    "    \n",
    "    Args:\n",
    "        df: pyspark dataframe with data to be stratified sampled\n",
    "        fractions: a dictionary of fractions for each category in the categorical variable\n",
    "        categorical_class: the variable on which to perform stratified sampling\n",
    "        random_state: default = 42. Set the seed for reproducibility\n",
    "    Returns:\n",
    "        df: a pyspark dataframe which has been stratified sampled based on the above criteria.\n",
    "    \"\"\"\n",
    "    auto_fractions = df.select(\"{}\".format(categorical_class)).distinct().withColumn(\"fraction\", lit(1.0)).rdd.collectAsMap()\n",
    "    #fractions = {'safe': 0.1, 'unsafe': 1.0, 'vandal':1.0}\n",
    "    # override default 1.0 non-samples with classes which need to be subsampled\n",
    "    for frac in fractions.items():\n",
    "        key = frac[0]\n",
    "        frac_value = frac[1]\n",
    "        auto_fractions[key] = frac_value\n",
    "    \n",
    "    seed = random_state\n",
    "    sampled_df = df.stat.sampleBy(categorical_class, auto_fractions, seed)\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe|  675|\n",
      "|unsafe|  126|\n",
      "|vandal|    9|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|unsafe|  126|\n",
      "|  safe|   96|\n",
      "|vandal|    9|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_df = get_stratified_sample(df = final_df, fractions = {'safe': 0.15})\n",
    "sampled_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split in training and test set\n",
    "\n",
    "preserve balance of classes by performing a **stratisfied split** to get a representive test set\n",
    "\n",
    "### IMPORTANT\n",
    "- The **training set** is to train the model and find good paramters (possible an additional validation test or cross validation)\n",
    "- The **test set** is to evaluate performance (generalization error) of the final chosen model\n",
    "- When you stream the data: that's model deployment (I don't really consider this as test data), this is actual live incoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 80% of both 0's and 1's into training set\n",
    "split_ratio = 0.8\n",
    "train = sampled_df.sampleBy(\"label\", \n",
    "                            fractions={'unsafe': split_ratio, 'safe': split_ratio, 'vandal':split_ratio}, seed=10)\n",
    "\n",
    "# Subtracting 'train' from original 'data' to get test set \n",
    "test = sampled_df.subtract(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|unsafe|  102|\n",
      "|  safe|   75|\n",
      "|vandal|    7|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|unsafe|   24|\n",
      "|  safe|   21|\n",
      "|vandal|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TF-IDF Features on New and Old Texts \n",
    "\n",
    "Term Frequency - Inverse Document Frequency (TF-IDF) is a technique used to build features out of text documents which have theoretically infinite dimensionality without feature reduction techniques such as this. The term-frequency is the step where we take the tokenized words from the text documents and hash them to a finite feature space. The resulting vectors represent a single document of text. For example, the text 'the brown fox' will hash to a vector of specified length, say 5, such that the result of the hash yields [1,0,2,0,0]. In the case of Spark, the hash used is MurmurHash 3.\n",
    "\n",
    "However, in a large text corpus, some words will be very present (e.g. “the”, “a”, “is”) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to incorporate the document frequency of occurrence as a weight or normalization to the term-frequencies mentioned above. Hence, TF-IDF.\n",
    "\n",
    "### Function below is depreciated (will be removed in further updates)\n",
    " - the function below is integrated in the pipeline (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfIdf(df=None, \n",
    "          text_col_for_tf_idf=None, \n",
    "          output_tf_idf_col=None, \n",
    "          numFeatures=20, \n",
    "          count_method ='hash'):\n",
    "    \n",
    "    \"\"\" This fucntion takes the text data and converts it into a term frequency-Inverse Document Frequency vector\n",
    "        The steps for this are tokenization of the input string column, stop word removal, feature hashing/count vectorization depending on \n",
    "        the count_method, and inverse document normalization step.\n",
    "        \n",
    "    Args: \n",
    "        - text_col_for_tf_idf: input text column of type 'string' in Java which is used as input to the tokenization, stop word removal and TF-IDF step\n",
    "        - output_tf_idf_col: output column to store the resulting feature\n",
    "        - count_method: default = 'hash'. Determines whether to use featuer hashing or counts as the TF step for TF-IDF\n",
    "    returns: \n",
    "        dataframe with tf-idf vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Carrying out the Tokenization of the text documents (splitting into words)\n",
    "    tokenizer = Tokenizer(inputCol=text_col_for_tf_idf, outputCol=\"tokenised_text\")\n",
    "    tokensDf = tokenizer.transform(df)\n",
    "    # Carrying out the StopWords Removal for TF-IDF\n",
    "    stopwordsremover=StopWordsRemover(inputCol='tokenised_text',outputCol='words')\n",
    "    swremovedDf= stopwordsremover.transform(tokensDf)\n",
    "\n",
    "    if count_method == 'hash':\n",
    "        # hashing is irreversible whereas counting is \n",
    "        # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "        # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "        hashingTF = HashingTF(inputCol=\"words\", outputCol=\"tf_features\", numFeatures=20)\n",
    "        tfDf = hashingTF.transform(swremovedDf)\n",
    "    else:\n",
    "        # Creating Term Frequency Vector for each word\n",
    "        cv=CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=300, minDF=2.0)\n",
    "        cvModel=cv.fit(swremovedDf)\n",
    "        tfDf=cvModel.transform(swremovedDf)\n",
    "\n",
    "    # Carrying out Inverse Document Frequency on the TF data\n",
    "    # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "    # which occur in less than a minimum number of documents.\n",
    "    # In such cases, the IDF for these terms is set to 0.\n",
    "    # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "    idf=IDF(inputCol=\"tf_features\", outputCol=\"{}\".format(output_tf_idf_col))\n",
    "    idfModel = idf.fit(tfDf)\n",
    "    tfidfDf = idfModel.transform(tfDf)\n",
    "\n",
    "    tfidfDf.cache().count()\n",
    "    cols_to_drop = [\"tokenised_text\", \"tf_features\", \"words\"]\n",
    "    tfidfDf = tfidfDf.drop(*cols_to_drop)\n",
    "\n",
    "    return tfidfDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF via Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidfDf = tfIdf(sampled_df, text_col_for_tf_idf = \"clean_new_text\", output_tf_idf_col = \"new_text_tf_idf_features\")\n",
    "# tfidfDf = tfIdf(tfidfDf, text_col_for_tf_idf = \"clean_old_text\", output_tf_idf_col = \"old_text_tf_idf_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. N-Gram Features on Text Differences (Added/Removed)\n",
    "\n",
    "Here we extract n-gram features from the text differences (text added or removed). The goal is from these simple combinations of words to extract usable features for modelling. Since the words are unordered, an n-gram model is appropriate, as it itself is not necessarily order-preserving in its selection of words.\n",
    "\n",
    "We select $n = 2$ for simplicity of the method. Additionally, we optionally apply feature hashing to the resulting n-grams.\n",
    "\n",
    "\n",
    "### Function below is depreciated (will be removed in further updates)\n",
    " - the function below is integrated in the pipeline (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(df, text_col_for_ngrams, output_col_for_ngrams, n = 2, do_feature_hashing = True):\n",
    "    \"\"\" This function takes a text column and converts it to a (hashed or unhashed) n-gram representation.\n",
    "        The steps are to remove stop words and to run the n-gram, then do optional feature hashing.\n",
    "        \n",
    "    parameter: \n",
    "        text_col_for_ngrams: input text column of typ 'string' in Java which is used as input to the stop word removal and n-gram step\n",
    "        output_col_for_ngrams: output column to store the resulting feature\n",
    "        n: default = 2. Determines the value of n for the n-gram calculation. Example, n = 1 is a unigram of single words.\n",
    "        do_feature_hashing: default = True. Determines whether to use featuer hashing or not\n",
    "    returns: dataframe with n-gram vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ngram = NGram(n=n, inputCol=\"{}\".format(text_col_for_ngrams), outputCol=\"ngrams\")\n",
    "    df = ngram.transform(df)\n",
    "    if do_feature_hashing:\n",
    "        # Carrying out the StopWords Removal for TF-IDF\n",
    "        stopwordsremover=StopWordsRemover(inputCol='ngrams',outputCol='words')\n",
    "        swremovedDf= stopwordsremover.transform(df)\n",
    "        # hashing is irreversible whereas counting is \n",
    "        # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "        # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "        hashingTF = HashingTF(inputCol=\"words\", outputCol=\"{}\".format(output_col_for_ngrams), numFeatures=20)\n",
    "        tfDf = hashingTF.transform(swremovedDf)  \n",
    "        tfDf = tfDf.drop(\"words\")\n",
    "    tfDf = tfDf.drop(\"ngrams\")\n",
    "    return tfDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate N-Gram Features via Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"removed_words\", \n",
    "#                          output_col_for_ngrams = \"removed_words_ngrams_hash_features\")\n",
    "# tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"added_words\", \n",
    "#                          output_col_for_ngrams = \"added_words_ngrams_hash_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 construct pipeline\n",
    "\n",
    " - Spark API pipeline: https://spark.apache.org/docs/latest/ml-pipeline.html (very similar to scikit-learn)\n",
    " - Spark API extact features (e.g. *TF-IDF, N-Gram*): https://spark.apache.org/docs/latest/ml-features.html \n",
    " - Spark API models: https://spark.apache.org/docs/latest/ml-classification-regression.html \n",
    " \n",
    "**TODO**:  \n",
    " - Try out different feature engineering steps and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1  TF-IDF Features (new and old text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A) clean_new_text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrying out the Tokenization of the text documents (splitting into words)\n",
    "tokenizer_new = Tokenizer(inputCol=\"clean_new_text\", outputCol=\"clean_new_tokenised_text\")\n",
    "stopwordsremover_new=StopWordsRemover(inputCol=tokenizer_new.getOutputCol(),outputCol='words_clean_new')\n",
    "# hashing is irreversible whereas counting is \n",
    "hashingTF_new = HashingTF(inputCol=stopwordsremover_new.getOutputCol(), outputCol=\"tf_features_clean_new\", numFeatures=100)\n",
    "# cv=CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=300, minDF=2.0)\n",
    "idf_new = IDF(inputCol=hashingTF_new.getOutputCol(), outputCol=\"feature_clean_new\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *B) clean_old_text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrying out the Tokenization of the text documents (splitting into words)\n",
    "tokenizer_old = Tokenizer(inputCol=\"clean_old_text\", outputCol=\"clean_old_tokenised_text\")\n",
    "stopwordsremover_old=StopWordsRemover(inputCol=tokenizer_old.getOutputCol(),outputCol='words_clean_old')\n",
    "# hashing is irreversible whereas counting is \n",
    "hashingTF_old = HashingTF(inputCol=stopwordsremover_old.getOutputCol(), outputCol=\"tf_features_clean_old\", numFeatures=100)\n",
    "# cv=CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=300, minDF=2.0)\n",
    "idf_old = IDF(inputCol=hashingTF_old.getOutputCol(), outputCol=\"feature_clean_old\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 N-Gram Features on Text Differences (Added/Removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A) added_words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_added = NGram(n=2, inputCol=\"added_words\", outputCol=\"ngrams_added\")\n",
    "# Carrying out the StopWords Removal for TF-IDF\n",
    "stopwordsremover_added=StopWordsRemover(inputCol=ngram_added.getOutputCol(),outputCol='words_added')\n",
    "hashingTF_added = HashingTF(inputCol=stopwordsremover_added.getOutputCol(), outputCol=\"feature_added\", numFeatures=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *B) removed_words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_removed = NGram(n=2, inputCol=\"removed_words\", outputCol=\"ngrams_removed\")\n",
    "# Carrying out the StopWords Removal for TF-IDF\n",
    "stopwordsremover_removed=StopWordsRemover(inputCol=ngram_removed.getOutputCol(),outputCol='words_removed')\n",
    "hashingTF_removed = HashingTF(inputCol=stopwordsremover_removed.getOutputCol(), outputCol=\"feature_removed\", numFeatures=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Add all steps and define model\n",
    "\n",
    "You can change the model to whathever model you want to try\n",
    "see SPARK API for all models:\n",
    "\n",
    "- Spark API models: https://spark.apache.org/docs/latest/ml-classification-regression.html \n",
    "\n",
    "### TODO \n",
    "- Explore different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all features to a vector assembler and call it features (default names for most models)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"feature_clean_new\",\"feature_clean_old\",\n",
    "               \"feature_removed\",\"feature_added\"],\n",
    "                outputCol=\"features\")\n",
    "\n",
    "# make target numeric\n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"target\")\n",
    "\n",
    "# define model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, \n",
    "                        featuresCol='features',\n",
    "                        labelCol='target')\n",
    "\n",
    "# add pipeline steps\n",
    "pipeline = Pipeline(stages=[tokenizer_new, stopwordsremover_new, hashingTF_new, idf_new, \n",
    "                            tokenizer_old, stopwordsremover_old, hashingTF_old, idf_old,\n",
    "                            ngram_removed, stopwordsremover_removed, hashingTF_removed,\n",
    "                            ngram_added, stopwordsremover_added, hashingTF_added,\n",
    "                            assembler, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Train define model (pipeline), evaluate, and find good parameters\n",
    "\n",
    "**TODO** \n",
    "  - find reasoanble parameters for chosen models (model tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+\n",
      "| label|         probability|prediction|\n",
      "+------+--------------------+----------+\n",
      "|unsafe|[0.99999999990567...|       0.0|\n",
      "|  safe|[4.25847421797823...|       1.0|\n",
      "|unsafe|[0.99999978834739...|       0.0|\n",
      "|  safe|[4.77315719487516...|       1.0|\n",
      "|  safe|[2.87856080558190...|       1.0|\n",
      "+------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.transform(train)\n",
    "pred_train.select(\"label\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9728260869565217\n",
      "FPR: 0.030780886688005292\n",
      "TPR: 0.9728260869565217\n",
      "F-measure: 0.9726670832530946\n",
      "Precision: 0.9732983068152813\n",
      "Recall: 0.9728260869565217\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = model.stages[16].summary\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Performance of final model (pipeline) on test set\n",
    "\n",
    "\n",
    "### Important\n",
    " - Only use this dataset once you decided on your final model.\n",
    " - This is only to get and idea of your models performance in real live (when we start streaming)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "?MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+\n",
      "|target|         probability|prediction|\n",
      "+------+--------------------+----------+\n",
      "|   0.0|[0.34823711252734...|       1.0|\n",
      "|   0.0|[0.77946260563692...|       0.0|\n",
      "|   1.0|[0.24516462637095...|       1.0|\n",
      "|   0.0|[0.00409218947671...|       1.0|\n",
      "|   0.0|[0.99980329842690...|       0.0|\n",
      "+------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5415860735009671"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = model.transform(test)\n",
    "pred_test.select(\"target\",\"probability\", \"prediction\").show(5)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='target')\n",
    "evaluator.evaluate(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Aread under ROC score is :  0.5978260869565217\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "results = pred_test.select(['probability', 'target'])\n",
    " \n",
    "## prepare score-label set\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"Test Data Aread under ROC score is : \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Train final model (pipline) on all data and save\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "- train your final model on **ALL DATA** using the parameters found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = pipeline.fit(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save(\"../output/models/logistic_regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
